import numpy as np
import h5py
import matplotlib.pyplot as plt

# åŠ è½½æˆ‘ä»¬è‡ªå®šä¹‰çš„å·¥å…·å‡½æ•°
from testCases import *
from dnn_utils import *

# è®¾ç½®ä¸€äº›ç”»å›¾ç›¸å…³çš„å‚æ•°
plt.rcParams['figure.figsize'] = (5.0, 4.0)
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

np.random.seed(1)


# è¯¥å‡½æ•°ç”¨äºåˆå§‹åŒ–æ‰€æœ‰å±‚çš„å‚æ•°wå’Œb
def initialize_parameters_deep(layer_dims):
    """
    å‚æ•°:
    layer_dims -- è¿™ä¸ªliståˆ—è¡¨é‡Œé¢ï¼ŒåŒ…å«äº†æ¯å±‚çš„ç¥ç»å…ƒä¸ªæ•°ã€‚
    ä¾‹å¦‚ï¼Œlayer_dims=[5,4,3]ï¼Œè¡¨ç¤ºè¾“å…¥å±‚æœ‰5ä¸ªç¥ç»å…ƒï¼Œç¬¬ä¸€å±‚æœ‰4ä¸ªï¼Œæœ€åäºŒå±‚æœ‰3ä¸ªç¥ç»å…ƒ

    è¿”å›å€¼:
    parameters -- è¿™ä¸ªå­—å…¸é‡Œé¢åŒ…å«äº†æ¯å±‚å¯¹åº”çš„å·²ç»åˆå§‹åŒ–äº†çš„Wå’Œbã€‚
    ä¾‹å¦‚ï¼Œparameters['W1']è£…è½½äº†ç¬¬ä¸€å±‚çš„wï¼Œparameters['b1']è£…è½½äº†ç¬¬ä¸€å±‚çš„b
    """
    np.random.seed(1)
    parameters = {}
    L = len(layer_dims)  # è·å–ç¥ç»ç½‘ç»œæ€»å…±æœ‰å‡ å±‚

    # éå†æ¯ä¸€å±‚ï¼Œä¸ºæ¯ä¸€å±‚çš„Wå’Œbè¿›è¡Œåˆå§‹åŒ–
    for l in range(1, L):
        # æ„å»ºå¹¶éšæœºåˆå§‹åŒ–è¯¥å±‚çš„Wã€‚ç”±æˆ‘å‰é¢çš„æ–‡ç« ã€Š1.4.3 æ ¸å¯¹çŸ©é˜µçš„ç»´åº¦ã€‹å¯çŸ¥ï¼ŒWlçš„ç»´åº¦æ˜¯(n[l] , n[l-1])
        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) / np.sqrt(layer_dims[l - 1])
        # æ„å»ºå¹¶åˆå§‹åŒ–b
        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))

        # æ ¸å¯¹ä¸€ä¸‹Wå’Œbçš„ç»´åº¦æ˜¯æˆ‘ä»¬é¢„æœŸçš„ç»´åº¦
        assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))
        assert (parameters['b' + str(l)].shape == (layer_dims[l], 1))

    # å°±æ˜¯åˆ©ç”¨ä¸Šé¢çš„å¾ªç¯ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä¸ºä»»æ„å±‚æ•°çš„ç¥ç»ç½‘ç»œè¿›è¡Œå‚æ•°åˆå§‹åŒ–ï¼Œåªè¦æˆ‘ä»¬æä¾›æ¯ä¸€å±‚çš„ç¥ç»å…ƒä¸ªæ•°å°±å¯ä»¥äº†ã€‚
    return parameters


def initialize_parameters_deep_print():
    parameters = initialize_parameters_deep([5, 4, 3, 6])
    print("W1 = " + str(parameters["W1"]))
    print("b1 = " + str(parameters["b1"]))
    print("W2 = " + str(parameters["W2"]))
    print("b2 = " + str(parameters["b2"]))
    print("W3 = " + str(parameters["W3"]))
    print("b3 = " + str(parameters["b3"]))


# ä¸‹é¢å¼€å§‹æ„å»ºå‰å‘ä¼ æ’­æ‰€éœ€çš„å·¥å…·å‡½æ•°
#
# ä¸‹é¢çš„linear_forwardç”¨äºå®ç°å…¬å¼  ğ‘[ğ‘™]=ğ‘Š[ğ‘™]ğ´[ğ‘™âˆ’1]+ğ‘[ğ‘™] ï¼Œè¿™ä¸ªç§°ä¹‹ä¸ºçº¿æ€§å‰å‘ä¼ æ’­


def linear_forward(A, W, b):
    Z = np.dot(W, A) + b

    assert (Z.shape == (W.shape[0], A.shape[1]))
    cache = (A, W, b)  # å°†è¿™äº›å˜é‡ä¿å­˜èµ·æ¥ï¼Œå› ä¸ºåé¢è¿›è¡Œåå‘ä¼ æ’­æ—¶ä¼šç”¨åˆ°å®ƒä»¬

    return Z, cache


def linear_forward_print():
    A, W, b = linear_forward_test_case()

    Z, linear_cache = linear_forward(A, W, b)
    print("Z = " + str(Z))


# ä¸‹é¢çš„linear_activation_forwardç”¨äºå®ç°å…¬å¼  ğ´[ğ‘™]=ğ‘”(ğ‘[ğ‘™])
# ï¼Œgä»£è¡¨æ¿€æ´»å‡½æ•°ï¼Œä½¿ç”¨äº†æ¿€æ´»å‡½æ•°ä¹‹åä¸Šé¢çš„çº¿æ€§å‰å‘ä¼ æ’­å°±å˜æˆäº†éçº¿æ€§å‰å‘ä¼ æ’­äº†ã€‚åœ¨dnn_utils.pyä¸­æˆ‘ä»¬è‡ªå®šä¹‰äº†ä¸¤ä¸ªæ¿€æ´»å‡½æ•°ï¼Œsigmoidå’Œreluã€‚å®ƒä»¬éƒ½ä¼šæ ¹æ®ä¼ å…¥çš„Zè®¡ç®—å‡ºAã€‚


def linear_activation_forward(A_prev, W, b, activation):
    """
    Arguments:
    A_prev -- ä¸Šä¸€å±‚å¾—åˆ°çš„Aï¼Œè¾“å…¥åˆ°æœ¬å±‚æ¥è®¡ç®—Zå’Œæœ¬å±‚çš„Aã€‚ç¬¬ä¸€å±‚æ—¶A_prevå°±æ˜¯ç‰¹å¾è¾“å…¥X
    W -- æœ¬å±‚ç›¸å…³çš„W
    b -- æœ¬å±‚ç›¸å…³çš„b
    activation -- ä¸¤ä¸ªå­—ç¬¦ä¸²ï¼Œ"sigmoid"æˆ–"relu"ï¼ŒæŒ‡ç¤ºè¯¥å±‚åº”è¯¥ä½¿ç”¨å“ªç§æ¿€æ´»å‡½æ•°
    """

    Z, linear_cache = linear_forward(A_prev, W, b)

    if activation == "sigmoid":  # å¦‚æœè¯¥å±‚ä½¿ç”¨sigmoid
        A = sigmoid(Z)
    elif activation == "relu":
        A = relu(Z)

    assert (A.shape == (W.shape[0], A_prev.shape[1]))
    cache = (linear_cache, Z)  # ç¼“å­˜ä¸€äº›å˜é‡ï¼Œåé¢çš„åå‘ä¼ æ’­ä¼šç”¨åˆ°å®ƒä»¬

    return A, cache


def linear_activation_forward_test():
    A_prev, W, b = linear_activation_forward_test_case()

    A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation="sigmoid")
    print("With sigmoid: A = " + str(A))

    A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation="relu")
    print("With ReLU: A = " + str(A))


# è¿™ä¸ªå‡½æ•°æ„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ã€‚è¿™ä¸ªå‰å‘ä¼ æ’­ä¸€å…±æœ‰Lå±‚ï¼Œå‰é¢çš„L-1å±‚ç”¨çš„æ¿€æ´»å‡½æ•°æ˜¯reluï¼Œæœ€åä¸€å±‚ä½¿ç”¨sigmoidã€‚
def L_model_forward(X, parameters):
    """
    å‚æ•°:
    X -- è¾“å…¥çš„ç‰¹å¾æ•°æ®
    parameters -- è¿™ä¸ªliståˆ—è¡¨é‡Œé¢åŒ…å«äº†æ¯ä¸€å±‚çš„å‚æ•°wå’Œb
    """

    caches = []
    A = X

    # è·å–å‚æ•°åˆ—è¡¨çš„é•¿åº¦ï¼Œè¿™ä¸ªé•¿åº¦çš„ä¸€åŠå°±æ˜¯ç¥ç»ç½‘ç»œçš„å±‚æ•°ã€‚
    # ä¸ºä»€ä¹ˆæ˜¯ä¸€åŠå‘¢ï¼Ÿå› ä¸ºåˆ—è¡¨æ˜¯è¿™æ ·çš„[w1,b1,w2,b2...wl,bl],é‡Œé¢çš„w1å’Œb1ä»£è¡¨äº†ä¸€å±‚
    L = len(parameters) // 2

    # å¾ªç¯L-1æ¬¡ï¼Œå³è¿›è¡ŒL-1æ­¥å‰å‘ä¼ æ’­ï¼Œæ¯ä¸€æ­¥ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°éƒ½æ˜¯relu
    for l in range(1, L):
        A_prev = A
        A, cache = linear_activation_forward(A_prev,
                                             parameters['W' + str(l)],
                                             parameters['b' + str(l)],
                                             activation='relu')
        caches.append(cache)  # æŠŠä¸€äº›å˜é‡æ•°æ®ä¿å­˜èµ·æ¥ï¼Œä»¥ä¾¿åé¢çš„åå‘ä¼ æ’­ä½¿ç”¨

    # è¿›è¡Œæœ€åä¸€å±‚çš„å‰å‘ä¼ æ’­ï¼Œè¿™ä¸€å±‚çš„æ¿€æ´»å‡½æ•°æ˜¯sigmoidã€‚å¾—å‡ºçš„ALå°±æ˜¯y'é¢„æµ‹å€¼
    AL, cache = linear_activation_forward(A,
                                          parameters['W' + str(L)],
                                          parameters['b' + str(L)],
                                          activation='sigmoid')
    caches.append(cache)

    assert (AL.shape == (1, X.shape[1]))

    return AL, caches


def L_model_forward_test():
    X, parameters = L_model_forward_test_case()
    AL, caches = L_model_forward(X, parameters)
    print("AL = " + str(AL))
    print("Length of caches list = " + str(len(caches)))


# ä¸Šé¢å·²ç»å®Œæˆäº†å‰å‘ä¼ æ’­äº†ã€‚ä¸‹é¢è¿™ä¸ªå‡½æ•°ç”¨äºè®¡ç®—æˆæœ¬ï¼ˆå•ä¸ªæ ·æœ¬æ—¶æ˜¯æŸå¤±ï¼Œå¤šä¸ªæ ·æœ¬æ—¶æ˜¯æˆæœ¬ï¼‰ã€‚
# é€šè¿‡æ¯æ¬¡è®­ç»ƒçš„æˆæœ¬æˆ‘ä»¬å°±å¯ä»¥çŸ¥é“å½“å‰ç¥ç»ç½‘ç»œå­¦ä¹ çš„ç¨‹åº¦å¥½åã€‚
def compute_cost(AL, Y):
    m = Y.shape[1]
    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))

    cost = np.squeeze(cost)  # ç¡®ä¿costæ˜¯ä¸€ä¸ªæ•°å€¼è€Œä¸æ˜¯ä¸€ä¸ªæ•°ç»„çš„å½¢å¼
    assert (cost.shape == ())

    return cost


def compute_cost_test():
    Y, AL = compute_cost_test_case()

    print("cost = " + str(compute_cost(AL, Y)))


#  ä¸Šé¢å·²ç»å®ç°äº†å‰å‘ä¼ æ’­å’Œæˆæœ¬å‡½æ•°ï¼Œä¸‹é¢å¼€å§‹å®ç°åå‘ä¼ æ’­ã€‚é€šè¿‡åå‘ä¼ æ’­æ¥è®¡ç®—æ¢¯åº¦â€”â€”è®¡ç®—æ¯å±‚çš„wå’Œbç›¸å½“äºæˆæœ¬å‡½æ•°çš„åå¯¼æ•°ã€‚
# ä¸‹é¢çš„linear_backwardå‡½æ•°ç”¨äºæ ¹æ®åä¸€å±‚çš„dZæ¥è®¡ç®—å‰é¢ä¸€å±‚çš„dWï¼Œdbå’ŒdAã€‚ä¹Ÿå°±æ˜¯å®ç°äº†ä¸‹é¢3ä¸ªå…¬å¼
# ğ‘‘ğ‘Š[ğ‘™]=1ğ‘šğ‘‘ğ‘[ğ‘™]ğ´[ğ‘™âˆ’1]ğ‘‡
#
# ğ‘‘ğ‘[ğ‘™]=1ğ‘šâˆ‘ğ‘–=1ğ‘šğ‘‘ğ‘[ğ‘™](ğ‘–)
#
# ğ‘‘ğ´[ğ‘™âˆ’1]=ğ‘Š[ğ‘™]ğ‘‡ğ‘‘ğ‘[ğ‘™]


def linear_backward(dZ, cache):
    """
    å‚æ•°:
    dZ -- åé¢ä¸€å±‚çš„dZ
    cache -- å‰å‘ä¼ æ’­æ—¶æˆ‘ä»¬ä¿å­˜ä¸‹æ¥çš„å…³äºæœ¬å±‚çš„ä¸€äº›å˜é‡
    """
    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = np.dot(dZ, cache[0].T) / m
    db = np.sum(dZ, axis=1, keepdims=True) / m
    dA_prev = np.dot(cache[1].T, dZ)

    assert (dA_prev.shape == A_prev.shape)
    assert (dW.shape == W.shape)
    assert (db.shape == b.shape)

    return dA_prev, dW, db


def linear_backward_test():
    dZ, linear_cache = linear_backward_test_case()

    dA_prev, dW, db = linear_backward(dZ, linear_cache)
    print("dA_prev = " + str(dA_prev))
    print("dW = " + str(dW))
    print("db = " + str(db))


#
# ä¸‹é¢çš„linear_activation_backwardç”¨äºæ ¹æ®æœ¬å±‚çš„dAè®¡ç®—å‡ºæœ¬å±‚çš„dZã€‚å°±æ˜¯å®ç°äº†ä¸‹é¢çš„å…¬å¼
# ğ‘‘ğ‘[ğ‘™] = ğ‘‘ğ´[ğ‘™]âˆ—ğ‘”â€²(ğ‘[ğ‘™])
#
# ä¸Šå¼çš„g '()è¡¨ç¤ºæ±‚Zç›¸å½“äºæœ¬å±‚çš„æ¿€æ´»å‡½æ•°çš„åå¯¼æ•°ã€‚æ‰€ä»¥ä¸åŒçš„æ¿€æ´»å‡½æ•°ä¹Ÿæœ‰ä¸åŒçš„æ±‚å¯¼å…¬å¼ã€‚
# æˆ‘ä»¬ä¸ºå¤§å®¶ç¼–å†™äº†ä¸¤ä¸ªæ±‚å¯¼å‡½æ•°sigmoid_backwardå’Œrelu_backwardã€‚å¤§å®¶å½“å‰ä¸éœ€è¦å…³å¿ƒè¿™ä¸¤ä¸ªå‡½æ•°çš„å†…éƒ¨å®ç°ï¼Œå½“ç„¶ï¼Œå¦‚æœä½ æ„Ÿå…´è¶£å¯ä»¥åˆ°dnn_utils.pyé‡Œé¢å»çœ‹å®ƒä»¬çš„å®ç°ã€‚
#
#

def linear_activation_backward(dA, cache, activation):
    """
    å‚æ•°:
    dA -- æœ¬å±‚çš„dA
    cache -- å‰å‘ä¼ æ’­æ—¶ä¿å­˜çš„æœ¬å±‚çš„ç›¸å…³å˜é‡
    activation -- æŒ‡ç¤ºè¯¥å±‚ä½¿ç”¨çš„æ˜¯ä»€ä¹ˆæ¿€æ´»å‡½æ•°: "sigmoid" æˆ– "relu"
    """
    linear_cache, activation_cache = cache

    if activation == "relu":
        dZ = relu_backward(dA, activation_cache)
    elif activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache)

    # è¿™é‡Œæˆ‘ä»¬åˆé¡ºå¸¦æ ¹æ®æœ¬å±‚çš„dZç®—å‡ºæœ¬å±‚çš„dWå’Œdbä»¥åŠå‰ä¸€å±‚çš„dA
    dA_prev, dW, db = linear_backward(dZ, linear_cache)

    return dA_prev, dW, db


# ä¸‹é¢è¿™ä¸ªå‡½æ•°æ„å»ºå‡ºæ•´ä¸ªåå‘ä¼ æ’­ã€‚
def L_model_backward(AL, Y, caches):
    """
    å‚æ•°:
    AL -- æœ€åä¸€å±‚çš„Aï¼Œä¹Ÿå°±æ˜¯y'ï¼Œé¢„æµ‹å‡ºçš„æ ‡ç­¾
    Y -- çœŸå®æ ‡ç­¾
    caches -- å‰å‘ä¼ æ’­æ—¶ä¿å­˜çš„æ¯ä¸€å±‚çš„ç›¸å…³å˜é‡ï¼Œç”¨äºè¾…åŠ©è®¡ç®—åå‘ä¼ æ’­
    """
    grads = {}
    L = len(caches)  # è·å–ç¥ç»ç½‘ç»œå±‚æ•°ã€‚cachesåˆ—è¡¨çš„é•¿åº¦å°±ç­‰äºç¥ç»ç½‘ç»œçš„å±‚æ•°
    Y = Y.reshape(AL.shape)  # è®©çœŸå®æ ‡ç­¾çš„ç»´åº¦å’Œé¢„æµ‹æ ‡ç­¾çš„ç»´åº¦ä¸€è‡´

    # è®¡ç®—å‡ºæœ€åä¸€å±‚çš„dAï¼Œå‰é¢æ–‡ç« æˆ‘ä»¬ä»¥åŠè§£é‡Šè¿‡ï¼Œæœ€åä¸€å±‚çš„dAä¸å‰é¢å„å±‚çš„dAçš„è®¡ç®—å…¬å¼ä¸åŒï¼Œ
    # å› ä¸ºæœ€åä¸€ä¸ªAæ˜¯ç›´æ¥ä½œä¸ºå‚æ•°ä¼ é€’åˆ°æˆæœ¬å‡½æ•°çš„ï¼Œæ‰€ä»¥ä¸éœ€è¦é“¾å¼æ³•åˆ™è€Œç›´æ¥å°±å¯ä»¥æ±‚dAï¼ˆAç›¸å½“äºæˆæœ¬å‡½æ•°çš„åå¯¼æ•°ï¼‰
    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

    # è®¡ç®—æœ€åä¸€å±‚çš„dWå’Œdbï¼Œå› ä¸ºæœ€åä¸€å±‚ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°æ˜¯sigmoid
    current_cache = caches[-1]
    grads["dA" + str(L - 1)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(
        dAL,
        current_cache,
        activation="sigmoid")

    # è®¡ç®—å‰é¢L-1å±‚åˆ°ç¬¬ä¸€å±‚çš„æ¯å±‚çš„æ¢¯åº¦ï¼Œè¿™äº›å±‚éƒ½ä½¿ç”¨reluæ¿€æ´»å‡½æ•°
    for c in reversed(range(1, L)):  # reversed(range(1,L))çš„ç»“æœæ˜¯L-1,L-2...1ã€‚æ˜¯ä¸åŒ…æ‹¬Lçš„ã€‚ç¬¬0å±‚æ˜¯è¾“å…¥å±‚ï¼Œä¸å¿…è®¡ç®—ã€‚
        # è¿™é‡Œçš„cè¡¨ç¤ºå½“å‰å±‚
        grads["dA" + str(c - 1)], grads["dW" + str(c)], grads["db" + str(c)] = linear_activation_backward(
            grads["dA" + str(c)],
            caches[c - 1],
            # è¿™é‡Œæˆ‘ä»¬ä¹Ÿæ˜¯éœ€è¦å½“å‰å±‚çš„cachesï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆæ˜¯c-1å‘¢ï¼Ÿå› ä¸ºgradsæ˜¯å­—å…¸ï¼Œæˆ‘ä»¬ä»1å¼€å§‹è®¡æ•°ï¼Œè€Œcachesæ˜¯åˆ—è¡¨ï¼Œ
            # æ˜¯ä»0å¼€å§‹è®¡æ•°ã€‚æ‰€ä»¥c-1å°±ä»£è¡¨äº†cå±‚çš„cachesã€‚æ•°ç»„çš„ç´¢å¼•å¾ˆå®¹æ˜“å¼•èµ·è«åå…¶å¦™çš„é—®é¢˜ï¼Œå¤§å®¶ç¼–ç¨‹æ—¶ä¸€å®šè¦ç•™æ„ã€‚
            activation="relu")

    return grads


# é€šè¿‡ä¸Šé¢çš„åå‘ä¼ æ’­ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ¯ä¸€å±‚çš„æ¢¯åº¦ï¼ˆæ¯ä¸€å±‚wå’Œbç›¸å½“äºæˆæœ¬å‡½æ•°çš„åå¯¼æ•°ï¼‰ã€‚ä¸‹é¢çš„update_parameterså‡½æ•°å°†åˆ©ç”¨è¿™äº›æ¢¯åº¦æ¥æ›´æ–°/ä¼˜åŒ–æ¯ä¸€å±‚çš„wå’Œbï¼Œä¹Ÿå°±æ˜¯è¿›è¡Œæ¢¯åº¦ä¸‹é™ã€‚
# ğ‘Š[ğ‘™]=ğ‘Š[ğ‘™]âˆ’ğ›¼ ğ‘‘ğ‘Š[ğ‘™]
#
# ğ‘[ğ‘™]=ğ‘[ğ‘™]âˆ’ğ›¼ ğ‘‘ğ‘[ğ‘™]


def L_model_backward_test():
    AL, Y_assess, caches = L_model_backward_test_case()
    grads = L_model_backward(AL, Y_assess, caches)
    print("dW1 = " + str(grads["dW1"]))
    print("db1 = " + str(grads["db1"]))
    print("dA1 = " + str(grads["dA1"]))


def linear_activation_backward_test():
    dAL, linear_activation_cache = linear_activation_backward_test_case()

    dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation="sigmoid")
    print("sigmoid:")
    print("dA_prev = " + str(dA_prev))
    print("dW = " + str(dW))
    print("db = " + str(db) + "\n")

    dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation="relu")
    print("relu:")
    print("dA_prev = " + str(dA_prev))
    print("dW = " + str(dW))
    print("db = " + str(db))


def update_parameters(parameters, grads, learning_rate):
    """
    Arguments:
    parameters -- æ¯ä¸€å±‚çš„å‚æ•°wå’Œb
    grads -- æ¯ä¸€å±‚çš„æ¢¯åº¦
    learning_rate -- æ˜¯å­¦ä¹ ç‡ï¼Œå­¦ä¹ æ­¥è¿›
    """

    L = len(parameters) // 2  # è·å–å±‚æ•°ã€‚//é™¤æ³•å¯ä»¥å¾—åˆ°æ•´æ•°

    for l in range(1, L + 1):
        parameters["W" + str(l)] = parameters["W" + str(l)] - learning_rate * grads["dW" + str(l)]
        parameters["b" + str(l)] = parameters["b" + str(l)] - learning_rate * grads["db" + str(l)]

    return parameters


def update_parameters_test():
    parameters, grads = update_parameters_test_case()
    parameters = update_parameters(parameters, grads, 0.1)

    print("W1 = " + str(parameters["W1"]))
    print("b1 = " + str(parameters["b1"]))
    print("W2 = " + str(parameters["W2"]))
    print("b2 = " + str(parameters["b2"]))


train_x_orig, train_y, test_x_orig, test_y, classes = load_data()

m_train = train_x_orig.shape[0]  # è®­ç»ƒæ ·æœ¬çš„æ•°é‡
m_test = test_x_orig.shape[0]  # æµ‹è¯•æ ·æœ¬çš„æ•°é‡
num_px = test_x_orig.shape[1]  # æ¯å¼ å›¾ç‰‡çš„å®½/é«˜

# ä¸ºäº†æ–¹ä¾¿åé¢è¿›è¡ŒçŸ©é˜µè¿ç®—ï¼Œæˆ‘ä»¬éœ€è¦å°†æ ·æœ¬æ•°æ®è¿›è¡Œæ‰å¹³åŒ–å’Œè½¬ç½®
# å¤„ç†åçš„æ•°ç»„å„ç»´åº¦çš„å«ä¹‰æ˜¯ï¼ˆå›¾ç‰‡æ•°æ®ï¼Œæ ·æœ¬æ•°ï¼‰
train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T
test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T

# ä¸‹é¢æˆ‘ä»¬å¯¹ç‰¹å¾æ•°æ®è¿›è¡Œäº†ç®€å•çš„æ ‡å‡†åŒ–å¤„ç†ï¼ˆé™¤ä»¥255ï¼Œä½¿æ‰€æœ‰å€¼éƒ½åœ¨[0ï¼Œ1]èŒƒå›´å†…ï¼‰
train_x = train_x_flatten / 255.
test_x = test_x_flatten / 255.


# åˆ©ç”¨ä¸Šé¢çš„å·¥å…·å‡½æ•°æ„å»ºä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒæ¨¡å‹
def dnn_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):
    """
    å‚æ•°:
    X -- æ•°æ®é›†
    Y -- æ•°æ®é›†æ ‡ç­¾
    layers_dims -- æŒ‡ç¤ºè¯¥æ·±åº¦ç¥ç»ç½‘ç»œç”¨å¤šå°‘å±‚ï¼Œæ¯å±‚æœ‰å¤šå°‘ä¸ªç¥ç»å…ƒ
    learning_rate -- å­¦ä¹ ç‡
    num_iterations -- æŒ‡ç¤ºéœ€è¦è®­ç»ƒå¤šå°‘æ¬¡
    print_cost -- æŒ‡ç¤ºæ˜¯å¦éœ€è¦åœ¨å°†è®­ç»ƒè¿‡ç¨‹ä¸­çš„æˆæœ¬ä¿¡æ¯æ‰“å°å‡ºæ¥ï¼Œå¥½çŸ¥é“è®­ç»ƒçš„è¿›åº¦å¥½åã€‚

    è¿”å›å€¼:
    parameters -- è¿”å›è®­ç»ƒå¥½çš„å‚æ•°ã€‚ä»¥åå°±å¯ä»¥ç”¨è¿™äº›å‚æ•°æ¥è¯†åˆ«æ–°çš„é™Œç”Ÿçš„å›¾ç‰‡
    """

    np.random.seed(1)
    costs = []

    # åˆå§‹åŒ–æ¯å±‚çš„å‚æ•°wå’Œb
    parameters = initialize_parameters_deep(layers_dims)

    # æŒ‰ç…§æŒ‡ç¤ºçš„æ¬¡æ•°æ¥è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œ
    for i in range(0, num_iterations):
        # è¿›è¡Œå‰å‘ä¼ æ’­
        AL, caches = L_model_forward(X, parameters)
        # è®¡ç®—æˆæœ¬
        cost = compute_cost(AL, Y)
        # è¿›è¡Œåå‘ä¼ æ’­
        grads = L_model_backward(AL, Y, caches)
        # æ›´æ–°å‚æ•°ï¼Œå¥½ç”¨è¿™äº›å‚æ•°è¿›è¡Œä¸‹ä¸€è½®çš„å‰å‘ä¼ æ’­
        parameters = update_parameters(parameters, grads, learning_rate)

        # æ‰“å°å‡ºæˆæœ¬
        if i % 100 == 0:
            if print_cost and i > 0:
                print("è®­ç»ƒ%iæ¬¡åæˆæœ¬æ˜¯: %f" % (i, cost))
            costs.append(cost)

    # ç”»å‡ºæˆæœ¬æ›²çº¿å›¾
    plt.plot(np.squeeze(costs))
    plt.ylabel('cost')
    plt.xlabel('iterations (per tens)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()

    return parameters


# è®¾ç½®å¥½æ·±åº¦ç¥ç»ç½‘ç»œçš„å±‚æ¬¡ä¿¡æ¯â€”â€”ä¸‹é¢ä»£è¡¨äº†ä¸€ä¸ª4å±‚çš„ç¥ç»ç½‘ç»œï¼ˆ12288æ˜¯è¾“å…¥å±‚ï¼‰ï¼Œ
# ç¬¬ä¸€å±‚æœ‰20ä¸ªç¥ç»å…ƒï¼Œç¬¬äºŒå±‚æœ‰7ä¸ªç¥ç»å…ƒã€‚ã€‚ã€‚
# ä½ ä¹Ÿå¯ä»¥æ„å»ºä»»æ„å±‚ä»»æ„ç¥ç»å…ƒæ•°é‡çš„ç¥ç»ç½‘ç»œï¼Œåªéœ€è¦æ›´æ”¹ä¸‹é¢è¿™ä¸ªæ•°ç»„å°±å¯ä»¥äº†
layers_dims = [12288, 20, 7, 5, 1]

# æ ¹æ®ä¸Šé¢çš„å±‚æ¬¡ä¿¡æ¯æ¥æ„å»ºä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œï¼Œå¹¶ä¸”ç”¨ä¹‹å‰åŠ è½½çš„æ•°æ®é›†æ¥è®­ç»ƒè¿™ä¸ªç¥ç»ç½‘ç»œï¼Œå¾—å‡ºè®­ç»ƒåçš„å‚æ•°
parameters = dnn_model(train_x, train_y, layers_dims, num_iterations=2000, print_cost=True)


def dnn_model_test():
    # è®¾ç½®å¥½æ·±åº¦ç¥ç»ç½‘ç»œçš„å±‚æ¬¡ä¿¡æ¯â€”â€”ä¸‹é¢ä»£è¡¨äº†ä¸€ä¸ª4å±‚çš„ç¥ç»ç½‘ç»œï¼ˆ12288æ˜¯è¾“å…¥å±‚ï¼‰ï¼Œ
    # ç¬¬ä¸€å±‚æœ‰20ä¸ªç¥ç»å…ƒï¼Œç¬¬äºŒå±‚æœ‰7ä¸ªç¥ç»å…ƒã€‚ã€‚ã€‚
    # ä½ ä¹Ÿå¯ä»¥æ„å»ºä»»æ„å±‚ä»»æ„ç¥ç»å…ƒæ•°é‡çš„ç¥ç»ç½‘ç»œï¼Œåªéœ€è¦æ›´æ”¹ä¸‹é¢è¿™ä¸ªæ•°ç»„å°±å¯ä»¥äº†
    layers_dims = [12288, 20, 7, 5, 1]

    # æ ¹æ®ä¸Šé¢çš„å±‚æ¬¡ä¿¡æ¯æ¥æ„å»ºä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œï¼Œå¹¶ä¸”ç”¨ä¹‹å‰åŠ è½½çš„æ•°æ®é›†æ¥è®­ç»ƒè¿™ä¸ªç¥ç»ç½‘ç»œï¼Œå¾—å‡ºè®­ç»ƒåçš„å‚æ•°
    parameters = dnn_model(train_x, train_y, layers_dims, num_iterations=2000, print_cost=True)


def predict(X, parameters):
    m = X.shape[1]
    n = len(parameters) // 2  # number of layers in the neural network
    p = np.zeros((1, m))

    # è¿›è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­ï¼Œå¾—åˆ°é¢„æµ‹ç»“æœ
    probas, caches = L_model_forward(X, parameters)

    # å°†é¢„æµ‹ç»“æœè½¬åŒ–æˆ0å’Œ1çš„å½¢å¼ï¼Œå³å¤§äº0.5çš„å°±æ˜¯1ï¼Œå¦åˆ™å°±æ˜¯0
    for i in range(0, probas.shape[1]):
        if probas[0, i] > 0.5:
            p[0, i] = 1
        else:
            p[0, i] = 0

    return p


if __name__ == '__main__':
    # initialize_parameters_deep_print()
    # linear_forward_print()

    # linear_activation_forward_test()
    # L_model_forward_test()
    # compute_cost_test()
    # linear_backward_test()
    # linear_activation_backward_test()
    # L_model_backward_test()
    # update_parameters_test()
    dnn_model_test()
    # å¯¹è®­ç»ƒæ•°æ®é›†è¿›è¡Œé¢„æµ‹
    pred_train = predict(train_x, parameters)
    print("é¢„æµ‹å‡†ç¡®ç‡æ˜¯: " + str(np.sum((pred_train == train_y) / train_x.shape[1])))

    # å¯¹æµ‹è¯•æ•°æ®é›†è¿›è¡Œé¢„æµ‹
    pred_test = predict(test_x, parameters)
    print("é¢„æµ‹å‡†ç¡®ç‡æ˜¯: " + str(np.sum((pred_test == test_y) / test_x.shape[1])))
